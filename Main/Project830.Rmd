---
title: "STAT 430/830: Project"
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
- \usepackage{amsmath}
- \usepackage{bm}
- \usepackage{arydshln}
- \usepackage{multirow}
- \usepackage{mathtools}
urlcolor: blue
---

```{r, include = FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=70),tidy=TRUE)
```
```{r}
library(tidyverse)
#setwd("/Users/aatrayee/Documents/STAT 844")

dataDirectory <- "./"
#phase1data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-13.csv"), stringsAsFactors = TRUE)
phase1data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-13.csv"), stringsAsFactors = TRUE)

#wish_data <- read.csv(file.path(dataDirectory, "wish.csv"))
#RESULTS_20837567_2021-08-13
phase1data
```

PHASE I
Convert low and high values to -1 and 1 respectively
```{r}
ts <- factor(phase1data$Tile.Size, levels = c(0.1,0.3), labels = c("0.1", "0.3")) #x1
ms <- factor(phase1data$Match.Score, levels = c(80,100), labels = c("80", "100")) #x2
pl <- factor(phase1data$Prev.Length, levels = c(100,120), labels = c("100", "120")) #x3
pt <- factor(phase1data$Prev.Type, levels = c("TT","AC"), labels = c("TT", "AC")) #x4
y <- phase1data$Browse.Time
```
```{r}
x1 <- ifelse(phase1data$Tile.Size == 0.1, -1, 1)
x2 <- ifelse(phase1data$Match.Score == 80, -1, 1)
x3 <- ifelse(phase1data$Prev.Length == 100, -1, 1)
x4 <- ifelse(phase1data$Prev.Type == "TT", -1, 1)
y <- phase1data$Browse.Time

```
```{r}
## Try fitting the full model with all 2^8 terms that would be of interest
#model.full <- lm(y~(Tile.Size+Match.Score+Prev.Length+Prev.Type)^4, data = phase1data) 
#model.try <- lm(y~(Tile.Size+Match.Score+Prev.Length+Prev.Type), data = phase1data) 

model.full <- lm(y~(x1+x2+x3+x4), data = phase1data) 
model.try <- lm(y~(x1+x2+x3+x4)^4, data = phase1data) 

summary(model.full)
summary(model.try)


```

Tile size is not significant from fitting the models to check for significant 
factors

Main effect plots:
```{r}
library(gplots)

par(mfrow=c(2,2)) 
plotmeans(formula = y~Match.Score, ylab = "Browse Time", xlab = "Match Score", data = phase1data, xaxt = "n", pch = 16)
axis(side = 1, at = c(1,2), labels = c("80", "100"))
plotmeans(formula = y~Prev.Length, ylab = "Browse Time", xlab = "Preview Length", data = phase1data, xaxt = "n", pch = 16)
axis(side = 1, at = c(1,2), labels = c("100", "120"))
plotmeans(formula = y~Prev.Type, ylab = "Browse Time", xlab = "Preview Type", data = phase1data, xaxt = "n", pch = 16)
axis(side = 1, at = c(1,2), labels = c("AC","TT"))


```

```{r}
boxplot(y~Prev.Type,data = phase1data)
```


```{r}
## Path of Steepest Descent Example

# Function to create blues
blue_palette <- colorRampPalette(c(rgb(247,251,255,maxColorValue = 255), rgb(8,48,107,maxColorValue = 255)))

# Function for converting from natural units to coded units
convert.N.to.C <- function(U,UH,UL){
  x <- (U - (UH+UL)/2) / ((UH-UL)/2)
  return(x)
}

# Function for converting from coded units to natural units
convert.C.to.N <- function(x,UH,UL){
  U <- x*((UH-UL)/2) + (UH+UL)/2
  return(U)
}

# Function to create x and y grids for contour plots 
mesh <- function(x, y) { 
  Nx <- length(x)
  Ny <- length(y)
  list(
    x = matrix(nrow = Nx, ncol = Ny, data = x),
    y = matrix(nrow = Nx, ncol = Ny, data = y, byrow = TRUE)
  )
}
```


```{r}
ttdata <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15TT.csv"), stringsAsFactors = TRUE)
step0data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15Step0.csv"), stringsAsFactors = TRUE)


## The factors and their low/center/high levels are as follows:
## Preview Length: 100  vs 110  vs 120
## Match.Score:   80 vs 90 vs 100
## The number of units in each of the 5 conditions is:
ttdata_step0 <- rbind(ttdata,step0data)
table(ttdata_step0$Prev.Length, ttdata_step0$Match.Score)
```

```{r}
## Determine whether we're close to the optimum to begin with
## (i.e, check whether the pure quadratic effect is significant)
ph1 <- data.frame(y = ttdata_step0$Browse.Time,
                  x1 = convert.N.to.C(U = ttdata_step0$Prev.Length, UH = 120, UL = 100),
                  x2 = convert.N.to.C(U = ttdata_step0$Match.Score, UH = 100, UL = 80))
ph1$xPQ <- (ph1$x1^2 + ph1$x2^2)/2
```


```{r}
## Check the average browsing time in each condition:
(aggregate(ph1$y, by = list(x1 = ph1$x1, x2 = ph1$x2), FUN = mean))

## The difference in average browsing time in factorial conditions vs. the center 
## point condition
mean(ph1$y[ph1$xPQ != 0]) - mean(ph1$y[ph1$xPQ == 0])


## Check to see if that's significant
m <- lm(y~x1+x2+x1*x2+xPQ, data = ph1)
summary(m)
```




```{r}
## It isn't, so we're in a flat area of the response surface. We should
## perform a steepest descent phase.

## Fit the first order model to determine the direction of the path of 
## steepest descent
m.fo <- lm(y~x1+x2, data = ph1)
beta0 <- coef(m.fo)[1]
beta1 <- coef(m.fo)[2]
beta2 <- coef(m.fo)[3]
grd <- mesh(x = seq(convert.N.to.C(U = 30, UH = 120, UL = 100), 
                    convert.N.to.C(U = 120, UH = 120, UL = 100), 
                    length.out = 100), 
            y = seq(convert.N.to.C(U = 0, UH = 100, UL = 80), 
                    convert.N.to.C(U = 100, UH = 100, UL = 80), 
                    length.out = 100))
x1 <- grd$x
x2 <- grd$y
eta.fo <- beta0 + beta1*x1 + beta2*x2
# 2D contour plot
contour(x = seq(convert.N.to.C(U = 30, UH = 120, UL = 100), 
                    convert.N.to.C(U = 120, UH = 120, UL = 100), 
                    length.out = 100), 
            y = seq(convert.N.to.C(U = 0, UH = 100, UL = 80), 
                    convert.N.to.C(U = 100, UH = 100, UL = 80), 
                    length.out = 100), 
        z = eta.fo, xlab = "x1 (Preview Length)", ylab = "x2 (Match Score)",
        nlevels = 15, col = blue_palette(15), labcex = 0.9, asp=1)
abline(a = 0, b = beta2/beta1, lty = 2)
points(x = 0, y = 0, col = "red", pch = 16)

## Calculate the coordinates along this path that we will experiment at

# The gradient vector
g <- matrix(c(beta1, beta2), nrow = 1)

# We will take steps of size 5 seconds in preview length. In coded units this is
PL.step <- convert.N.to.C(U = 110 + 5, UH = 120, UL = 100)
lamda <- PL.step/abs(beta1)

## Step 0: The center point we've already observed
x.old <- matrix(0, nrow=1, ncol=2)
text(x = 0, y = 0+0.25, labels = "0")
step0 <- data.frame(Prev.Length = convert.C.to.N(x = 0, UH = 120, UL = 100), 
                 Match.Score = convert.C.to.N(x = 0, UH = 100, UL = 80))
step0
x.old
step0_y <- data.frame(Avg = mean(step0data$Browse.Time))
step0_y

## Step 1: 
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "1")
step1 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))
step1
x.new
step1data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15Step1.csv"), stringsAsFactors = TRUE)
step1_y <- data.frame(Avg = mean(step1data$Browse.Time))
step1_y

## Step 2: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "2")
step2 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))
step2
x.new
step2data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15Step2.csv"), stringsAsFactors = TRUE)
step2_y <- data.frame(Avg = mean(step2data$Browse.Time))
step2_y

## Step 3: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "3")
step3 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))
step3
x.new
step3data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15Step3.csv"), stringsAsFactors = TRUE)
step3_y <- data.frame(Avg = mean(step3data$Browse.Time))
step3_y

## Step 4: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "4")
step4 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))
step4
x.new
step4data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15Step4.csv"), stringsAsFactors = TRUE)
step4_y <- data.frame(Avg = mean(step4data$Browse.Time))
step4_y

## Step 5: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "5")
step5 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))
step5
x.new
step5data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15Step5.csv"), stringsAsFactors = TRUE)
step5_y <- data.frame(Avg = mean(step5data$Browse.Time))
step5_y

## The following is a list of the conditions along the path of steepest descent
pstd.cond <- data.frame(Step = 0:5, rbind(step0, step1, step2, step3, step4, step5))
pstd.cond

pstd.y <- data.frame(y = 0:5, rbind(step0_y, step1_y, step2_y, step3_y, step4_y, step5_y))
pstd.y



plot(x = 0:5, y = pstd.y$Avg,
     type = "l", xlab = "Step Number", ylab = "Average Browsing Time")
points(x = 0:5, y = pstd.y$Avg,
       col = "red", pch = 16)

## Clearly average browsing time was minimized at Step 6
pstd.cond[pstd.cond$Step == 4,]


## We should follow this up with 2^2 factorial conditions to ensure we're close to optimum
## We will re-center our coded scale in this new region as follows:
##Prev.Length: 80 vs 90 vs 100
##Match.Score: 54 vs 67 vs 80

## Load this data and check whether the pure quadratic effect is significant
phase2tt <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15TTPhase2.csv"), stringsAsFactors = TRUE)
netflix.ph2.5 <- rbind(phase2tt,step4data)
ph2.5 <- data.frame(y = netflix.ph2.5$Browse.Time,
                  x1 = convert.N.to.C(U = netflix.ph2.5$Prev.Length, UH = 100, UL = 80),
                  x2 = convert.N.to.C(U = netflix.ph2.5$Match.Score, UH = 80, UL = 54))
ph2.5$xPQ <- (ph2.5$x1^2 + ph2.5$x2^2)/2

## Check the average browsing time in each condition:
aggregate(ph2.5$y, by = list(x1 = ph2.5$x1, x2 = ph2.5$x2), FUN = mean)

## The difference in average browsing time in factorial conditions vs. the center 
## point condition
mean(ph2.5$y[ph2.5$xPQ != 0]) - mean(ph2.5$y[ph2.5$xPQ == 0])

## Check to see if that's significant
m <- lm(y~x1+x2+x1*x2+xPQ, data = ph2.5)
summary(m)

## Yes, it is significant and so there is significant quadratic curvature in
## this region of the response surface. We should now commence phase 3 and 
## perform a respond surface design and fit a full second order model.



```

TT
Prev.Length: 80 vs 90 vs 100
Match.Score: 54 vs 67 vs 80



```{r}
samdata <- phase1data[phase1data$Prev.Type == "AC",]

samdata1 <- samdata[samdata$Tile.Size == 0.1,]

checking <- rbind(samdata1,step0data)

table(checking$Prev.Length, checking$Match.Score)

## Determine whether we're close to the optimum to begin with
## (i.e, check whether the pure quadratic effect is significant)
ph1 <- data.frame(y = checking$Browse.Time,
                  x1 = convert.N.to.C(U = checking$Prev.Length, UH = 120, UL = 100),
                  x2 = convert.N.to.C(U = checking$Match.Score, UH = 100, UL = 80))
ph1$xPQ <- (ph1$x1^2 + ph1$x2^2)/2

## Check the average browsing time in each condition:
(aggregate(ph1$y, by = list(x1 = ph1$x1, x2 = ph1$x2), FUN = mean))

## The difference in average browsing time in factorial conditions vs. the center 
## point condition
mean(ph1$y[ph1$xPQ != 0]) - mean(ph1$y[ph1$xPQ == 0])


## Check to see if that's significant
m <- lm(y~x1+x2+x1*x2+xPQ, data = ph1)
summary(m)

## It isn't, so we're in a flat area of the response surface. We should
## perform a steepest descent phase.

## Fit the first order model to determine the direction of the path of 
## steepest descent
m.fo <- lm(y~x1+x2, data = ph1)
beta0 <- coef(m.fo)[1]
beta1 <- coef(m.fo)[2]
beta2 <- coef(m.fo)[3]
grd <- mesh(x = seq(convert.N.to.C(U = 30, UH = 120, UL = 100), 
                    convert.N.to.C(U = 120, UH = 120, UL = 100), 
                    length.out = 100), 
            y = seq(convert.N.to.C(U = 0, UH = 100, UL = 80), 
                    convert.N.to.C(U = 100, UH = 100, UL = 80), 
                    length.out = 100))
x1 <- grd$x
x2 <- grd$y
eta.fo <- beta0 + beta1*x1 + beta2*x2
# 2D contour plot
contour(x = seq(convert.N.to.C(U = 30, UH = 120, UL = 100), 
                    convert.N.to.C(U = 120, UH = 120, UL = 100), 
                    length.out = 100), 
            y = seq(convert.N.to.C(U = 0, UH = 100, UL = 80), 
                    convert.N.to.C(U = 100, UH = 100, UL = 80), 
                    length.out = 100), 
        z = eta.fo, xlab = "x1 (Preview Length)", ylab = "x2 (Match Score)",
        nlevels = 15, col = blue_palette(15), labcex = 0.9, asp=1)
abline(a = 0, b = beta2/beta1, lty = 2)
points(x = 0, y = 0, col = "red", pch = 16)

## Calculate the coordinates along this path that we will experiment at

# The gradient vector
g <- matrix(c(beta1, beta2), nrow = 1)

# We will take steps of size 5 seconds in preview length. In coded units this is
PL.step <- convert.N.to.C(U = 110 + 5, UH = 120, UL = 100)
lamda <- PL.step/abs(beta1)

## Step 0: The center point we've already observed
x.old <- matrix(0, nrow=1, ncol=2)
text(x = 0, y = 0+0.25, labels = "0")
step0 <- data.frame(Prev.Length = convert.C.to.N(x = 0, UH = 120, UL = 100), 
                 Match.Score = convert.C.to.N(x = 0, UH = 100, UL = 80))
step0
x.old
step0_y <- data.frame(Avg = mean(step0data$Browse.Time))
step0_y

## Step 1: 
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "1")
step1 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))
step1
x.new
step1data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15Step1.csv"), stringsAsFactors = TRUE)
step1_y <- data.frame(Avg = mean(step1data$Browse.Time))
step1_y

## Step 2: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "2")
step2 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))
step2
x.new
step2data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15Step2.csv"), stringsAsFactors = TRUE)
step2_y <- data.frame(Avg = mean(step2data$Browse.Time))
step2_y

## Step 3: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "3")
step3 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))
step3
x.new
step3data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15Step3.csv"), stringsAsFactors = TRUE)
step3_y <- data.frame(Avg = mean(step3data$Browse.Time))
step3_y

## Step 4: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "4")
step4 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))
step4
x.new
step4data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15Step4.csv"), stringsAsFactors = TRUE)
step4_y <- data.frame(Avg = mean(step4data$Browse.Time))
step4_y

## Step 5: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "5")
step5 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))
step5
x.new
step5data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15Step5.csv"), stringsAsFactors = TRUE)
step5_y <- data.frame(Avg = mean(step5data$Browse.Time))
step5_y

## The following is a list of the conditions along the path of steepest descent
pstd.cond <- data.frame(Step = 0:5, rbind(step0, step1, step2, step3, step4, step5))
pstd.cond

pstd.y <- data.frame(y = 0:5, rbind(step0_y, step1_y, step2_y, step3_y, step4_y, step5_y))
pstd.y



plot(x = 0:5, y = pstd.y$Avg,
     type = "l", xlab = "Step Number", ylab = "Average Browsing Time")
points(x = 0:5, y = pstd.y$Avg,
       col = "red", pch = 16)

## Clearly average browsing time was minimized at Step 6
pstd.cond[pstd.cond$Step == 4,]


```




AC

```{r}
samdata <- phase1data[phase1data$Prev.Type == "AC",]

samdata1 <- samdata[samdata$Tile.Size == 0.1,]
step0ac_data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15ACStep0.csv"), stringsAsFactors = TRUE)

checking <- rbind(samdata1,step0ac_data)

table(checking$Prev.Length, checking$Match.Score)

## Determine whether we're close to the optimum to begin with
## (i.e, check whether the pure quadratic effect is significant)
ph1 <- data.frame(y = checking$Browse.Time,
                  x1 = convert.N.to.C(U = checking$Prev.Length, UH = 120, UL = 100),
                  x2 = convert.N.to.C(U = checking$Match.Score, UH = 100, UL = 80))
ph1$xPQ <- (ph1$x1^2 + ph1$x2^2)/2

## Check the average browsing time in each condition:
(aggregate(ph1$y, by = list(x1 = ph1$x1, x2 = ph1$x2), FUN = mean))

## The difference in average browsing time in factorial conditions vs. the center 
## point condition
mean(ph1$y[ph1$xPQ != 0]) - mean(ph1$y[ph1$xPQ == 0])


## Check to see if that's significant
m <- lm(y~x1+x2+x1*x2+xPQ, data = ph1)
summary(m)

## It isn't, so we're in a flat area of the response surface. We should
## perform a steepest descent phase.

## Fit the first order model to determine the direction of the path of 
## steepest descent
m.fo <- lm(y~x1+x2, data = ph1)
beta0 <- coef(m.fo)[1]
beta1 <- coef(m.fo)[2]
beta2 <- coef(m.fo)[3]
grd <- mesh(x = seq(convert.N.to.C(U = 30, UH = 120, UL = 100), 
                    convert.N.to.C(U = 120, UH = 120, UL = 100), 
                    length.out = 100), 
            y = seq(convert.N.to.C(U = 0, UH = 100, UL = 80), 
                    convert.N.to.C(U = 100, UH = 100, UL = 80), 
                    length.out = 100))
x1 <- grd$x
x2 <- grd$y
eta.fo <- beta0 + beta1*x1 + beta2*x2
# 2D contour plot
contour(x = seq(convert.N.to.C(U = 30, UH = 120, UL = 100), 
                    convert.N.to.C(U = 120, UH = 120, UL = 100), 
                    length.out = 100), 
            y = seq(convert.N.to.C(U = 0, UH = 100, UL = 80), 
                    convert.N.to.C(U = 100, UH = 100, UL = 80), 
                    length.out = 100), 
        z = eta.fo, xlab = "x1 (Preview Length)", ylab = "x2 (Match Score)",
        nlevels = 15, col = blue_palette(15), labcex = 0.9, asp=1)
abline(a = 0, b = beta2/beta1, lty = 2)
points(x = 0, y = 0, col = "red", pch = 16)

## Calculate the coordinates along this path that we will experiment at

# The gradient vector
g <- matrix(c(beta1, beta2), nrow = 1)

# We will take steps of size 5 seconds in preview length. In coded units this is
PL.step <- convert.N.to.C(U = 110 + 5, UH = 120, UL = 100)
lamda <- PL.step/abs(beta1)

## Step 0: The center point we've already observed
x.old <- matrix(0, nrow=1, ncol=2)
text(x = 0, y = 0+0.25, labels = "0")
step0 <- data.frame(Prev.Length = convert.C.to.N(x = 0, UH = 120, UL = 100), 
                 Match.Score = convert.C.to.N(x = 0, UH = 100, UL = 80))
step0
x.old
step0_y <- data.frame(Avg = mean(step0ac_data$Browse.Time))
step0_y

## Step 1: 
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "1")
step1 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))
step1
x.new
step1data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15ACStep1.csv"), stringsAsFactors = TRUE)
step1_y <- data.frame(Avg = mean(step1data$Browse.Time))
step1_y

## Step 2: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "2")
step2 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))
step2
x.new
step2data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15ACStep2.csv"), stringsAsFactors = TRUE)
step2_y <- data.frame(Avg = mean(step2data$Browse.Time))
step2_y

## Step 3: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "3")
step3 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))
step3
x.new
step3data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15ACStep3.csv"), stringsAsFactors = TRUE)
step3_y <- data.frame(Avg = mean(step3data$Browse.Time))
step3_y

## Step 4: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "4")
step4 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))
step4
x.new
step4data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15ACStep4.csv"), stringsAsFactors = TRUE)
step4_y <- data.frame(Avg = mean(step4data$Browse.Time))
step4_y

## Step 5: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "5")
step5 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))
step5
x.new
step5data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15ACStep5.csv"), stringsAsFactors = TRUE)
step5_y <- data.frame(Avg = mean(step5data$Browse.Time))
step5_y

## Step 6: 
x.old <- x.new
x.new <- x.old - lamda*g
points(x = x.new[1,1], y = x.new[1,2], col = "red", pch = 16)
text(x = x.new[1,1], y = x.new[1,2]+0.25, labels = "5")
step6 <- data.frame(Prev.Length = convert.C.to.N(x = x.new[1,1], UH = 120, UL = 100), 
                    Match.Score = round(convert.C.to.N(x = x.new[1,2], UH = 100, UL = 80)))
step6
x.new
step6data <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15ACStep6.csv"), stringsAsFactors = TRUE)
step6_y <- data.frame(Avg = mean(step6data$Browse.Time))
step6_y

## The following is a list of the conditions along the path of steepest descent
pstd.cond <- data.frame(Step = 0:6, rbind(step0, step1, step2, step3, step4, step5,step6))
pstd.cond

pstd.y <- data.frame(y = 0:6, rbind(step0_y, step1_y, step2_y, step3_y, step4_y, step5_y,step6_y))
pstd.y



plot(x = 0:6, y = pstd.y$Avg,
     type = "l", xlab = "Step Number", ylab = "Average Browsing Time")
points(x = 0:6, y = pstd.y$Avg,
       col = "red", pch = 16)

## Clearly average browsing time was minimized at Step 6
pstd.cond[pstd.cond$Step == 5,]


## We should follow this up with 2^2 factorial conditions to ensure we're close to optimum
## We will re-center our coded scale in this new region as follows:
##Prev.Length: 75 vs 85 vs 95
##Match.Score: 55 vs 65 vs 75

## Load this data and check whether the pure quadratic effect is significant
phase2ac <- read.csv(file.path(dataDirectory, "RESULTS_20837567_2021-08-15ACPhase2.csv"), stringsAsFactors = TRUE)
netflix.ph2.5 <- rbind(phase2ac,step5data)
ph2.5 <- data.frame(y = netflix.ph2.5$Browse.Time,
                  x1 = convert.N.to.C(U = netflix.ph2.5$Prev.Length, UH = 95, UL = 75),
                  x2 = convert.N.to.C(U = netflix.ph2.5$Match.Score, c)
ph2.5$xPQ <- (ph2.5$x1^2 + ph2.5$x2^2)/2

## Check the average browsing time in each condition:
aggregate(ph2.5$y, by = list(x1 = ph2.5$x1, x2 = ph2.5$x2), FUN = mean)

## The difference in average browsing time in factorial conditions vs. the center 
## point condition
mean(ph2.5$y[ph2.5$xPQ != 0]) - mean(ph2.5$y[ph2.5$xPQ == 0])

## Check to see if that's significant
m <- lm(y~x1+x2+x1*x2+xPQ, data = ph2.5)
summary(m)

## Yes, it is significant and so there is significant quadratic curvature in
## this region of the response surface. We should now commence phase 3 and 
## perform a respond surface design and fit a full second order model.



```

Prev.Length: 75 vs 85 vs 95
Match.Score: 55 vs 65 vs 75

## We should follow this up with 2^2 factorial conditions to ensure we're close to optimum
## We will re-center our coded scale in this new region as follows:
## Preview Length: 60  vs 75  vs 90
## Preview Size:   0.6 vs 0.7 vs 0.8

## Load this data and check whether the pure quadratic effect is significant
netflix.ph2.5 <- read.csv("2^2+cp_second_time.csv", header = TRUE)
ph2.5 <- data.frame(y = netflix.ph2.5$Browse.Time,
                  x1 = convert.N.to.C(U = netflix.ph2.5$Prev.Length, UH = 90, UL = 60),
                  x2 = convert.N.to.C(U = netflix.ph2.5$Prev.Size, UH = 0.8, UL = 0.6))
ph2.5$xPQ <- (ph2.5$x1^2 + ph2.5$x2^2)/2

## Check the average browsing time in each condition:
aggregate(ph2.5$y, by = list(x1 = ph2.5$x1, x2 = ph2.5$x2), FUN = mean)

## The difference in average browsing time in factorial conditions vs. the center 
## point condition
mean(ph2.5$y[ph2.5$xPQ != 0]) - mean(ph2.5$y[ph2.5$xPQ == 0])

## Check to see if that's significant
m <- lm(y~x1+x2+x1*x2+xPQ, data = ph2.5)
summary(m)

## Yes, it is significant and so there is significant quadratic curvature in
## this region of the response surface. We should now commence phase 3 and 
## perform a respond surface design and fit a full second order model.

PHASE 3

```{r}

## Central Composite Design Example

# Function to create blues
blue_palette <- colorRampPalette(c(rgb(247,251,255,maxColorValue = 255), rgb(8,48,107,maxColorValue = 255)))


# Function for converting from natural units to coded units
convert.N.to.C <- function(U,UH,UL){
  x <- (U - (UH+UL)/2) / ((UH-UL)/2)
  return(x)
}

# Function for converting from coded units to natural units
convert.C.to.N <- function(x,UH,UL){
  U <- x*((UH-UL)/2) + (UH+UL)/2
  return(U)
}

# Function to create x and y grids for contour plots 
mesh <- function(x, y) { 
  Nx <- length(x)
  Ny <- length(y)
  list(
    x = matrix(nrow = Nx, ncol = Ny, data = x),
    y = matrix(nrow = Nx, ncol = Ny, data = y, byrow = TRUE)
  )
}

phase3stest <- read.csv(file.path(dataDirectory, "RESULTS_20203083_2021-08-15phase3.csv"), stringsAsFactors = TRUE)

x1 <- ifelse(phase3stest$Prev.Length == 80, -1, ifelse(phase3stest$Prev.Length == 100,1,0))
x2 <- ifelse(phase3stest$Match.Score == 54, -1, ifelse(phase3stest$Match.Score == 80,1,0))
y <- phase3stest$Browse.Time

## We then fit the full 2nd-order response surface
model <- lm(y ~ x1 + x2 + x1*x2 + I(x1^2) + I(x2^2))
summary(model)

## Let's visualize this surface:
beta0 <- coef(model)[1]
beta1 <- coef(model)[2]
beta2 <- coef(model)[3]
beta12 <- coef(model)[6]
beta11 <- coef(model)[4]
beta22 <- coef(model)[5]
grd <- mesh(x = seq(convert.N.to.C(U = 0, UH = 95, UL = 75), 
                    convert.N.to.C(U = 100, UH = 95, UL = 75), 
                    length.out = 100), 
            y = seq(convert.N.to.C(U = 0, UH = 7, UL = 2), 
                    convert.N.to.C(U = 10, UH = 7, UL = 2), 
                    length.out = 100))
x1 <- grd$x
x2 <- grd$y
eta.so <- beta0 + beta1*x1 + beta2*x2 + beta12*x1*x2 + beta11*x1^2 + beta22*x2^2
pi.so <- exp(eta.so)/(1+exp(eta.so))
```
condition <- data.frame(x1 = convert.C.to.N(x = c(-1,-1,1,1,0,1,-1,0,0), UH = 95, UL = 75), 
                        x2 = convert.C.to.N(x = c(-1,1,-1,1,0,0,0,1,-1), UH = 75, UL = 55))

## Calculate the booking rate in each condition
pi_hat <- aggregate(x = lyft$y, by = list(condition.num = kronecker(1:9, rep(1, 500))), FUN = mean)
data.frame(Condition.Num = pi_hat$condition.num, 
           Disc.Amnt = condition$x1, 
           Disc.Dur = condition$x2,
           Booking.Rate = pi_hat$x)

## We then fit the full 2nd-order response surface
model <- lm(y ~ x1 + x2 + x1*x2 + I(x1^2) + I(x2^2), data = lyft)
summary(model)
```